{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 21\n",
      "\n",
      "Grenerating patches:\n",
      "|####################################################################################################|  145/145 100%\n",
      "\n",
      "Initial sample length per class:                                     46   1428 830  237  483  730  28   478  20   972  2455 593  205  1265 386  93   \n",
      "Number of training samples per class:                                 0    200  200  0    200  200  0    200  0    200  200  200  0    200  0    0    \n",
      "Length of testing data: 7434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import random\n",
    "import spectral\n",
    "import scipy.ndimage\n",
    "from skimage.transform import rotate\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "from config import config\n",
    "\n",
    "# IndianPines Dataset Preparation Without Augmentation\n",
    "# 0.25 for training and 0.75 for testing\n",
    "\n",
    "def report_progress(progress, total, lbar_prefix = '', rbar_prefix=''):\n",
    "    percent = int(progress / float(total) * 100)\n",
    "    buf = \"%s|%s|  %s%d/%d %s\"%(lbar_prefix, ('#' * percent).ljust(100, '-'),\n",
    "        rbar_prefix, progress, total, \"%d%%\"%(percent))\n",
    "    sys.stdout.write(buf)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def report_progress_done():\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "def Patch_TopLeft(height_index,width_index):\n",
    "    \"\"\"\n",
    "    Returns a mean-normalized patch, the top left corner of which \n",
    "    is at (height_index, width_index)\n",
    "    \n",
    "    Inputs: \n",
    "    height_index - row index of the top left corner of the image patch\n",
    "    width_index - column index of the top left corner of the image patch\n",
    "    \n",
    "    Outputs:\n",
    "    mean_normalized_patch - mean normalized patch of size (PATCH_SIZE, PATCH_SIZE) \n",
    "    whose top left corner is at (height_index, width_index)\n",
    "    \"\"\"\n",
    "    transpose_array = np.transpose(input_mat,(2,0,1))\n",
    "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
    "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
    "    patch = transpose_array[:, height_slice, width_slice]\n",
    "    mean_normalized_patch = []\n",
    "    for i in range(patch.shape[0]):\n",
    "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i]) \n",
    "    \n",
    "    return np.array(mean_normalized_patch)\n",
    "\n",
    "def Patch_Center(height_index,width_index):\n",
    "    \"\"\"\n",
    "    Returns a mean-normalized patch, the center corner of which \n",
    "    is at (height_index, width_index)\n",
    "    \n",
    "    Inputs: \n",
    "    height_index - row index of the top left corner of the image patch\n",
    "    width_index - column index of the top left corner of the image patch\n",
    "    \n",
    "    Outputs:\n",
    "    mean_normalized_patch - mean normalized patch of size (PATCH_SIZE, PATCH_SIZE) \n",
    "    whose top left corner is at (height_index, width_index)\n",
    "    \"\"\"\n",
    "    transpose_array = np.transpose(input_mat,(2,0,1))\n",
    "    patch = np.zeros((BAND, PATCH_SIZE, PATCH_SIZE))\n",
    "    offset = (PATCH_SIZE-1)//2\n",
    "    h_index = 0; w_index = 0\n",
    "    for h in range(height_index-offset, height_index+offset+1):\n",
    "        for w in range(width_index-offset, width_index+offset+1):\n",
    "            if h<0 or h>=HEIGHT or w<0 or w>=WIDTH:\n",
    "                continue\n",
    "            else:\n",
    "                patch[:,h-height_index+offset,w-width_index+offset] = transpose_array[:,h,w]\n",
    "    mean_normalized_patch = []\n",
    "    for i in range(patch.shape[0]):\n",
    "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i]) \n",
    "    return np.array(mean_normalized_patch)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set random seed\n",
    "    random.seed(config.indianPines_seed)\n",
    "\n",
    "    # Load dataset\n",
    "    DATA_PATH = os.path.join(os.getcwd(),\"Data\",config.dataset)\n",
    "    input_mat = scipy.io.loadmat(os.path.join(DATA_PATH, 'Indian_pines.mat'))['indian_pines']\n",
    "    target_mat = scipy.io.loadmat(os.path.join(DATA_PATH, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
    "    SAVE_PATH = os.path.join(DATA_PATH,config.patch_mode,\"patch_size{}_seed{}\".format(config.patch_size, str(config.indianPines_seed)))\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    # Define global variables\n",
    "    PATCH_SIZE = config.patch_size\n",
    "    HEIGHT = input_mat.shape[0]\n",
    "    WIDTH = input_mat.shape[1]\n",
    "    BAND = config.indianPines_band\n",
    "    TRAIN_PATCH,TRAIN_LABELS,TEST_PATCH,TEST_LABELS = [],[],[],[]\n",
    "    CLASSES = [] \n",
    "    COUNT = 200 # Number of patches of each class\n",
    "    OUTPUT_CLASSES = config.indianPines_class\n",
    "    TEST_FRAC = 0.25 # Fraction of data to be used for testing\n",
    "    print('patch_size: {}\\n'.format(PATCH_SIZE))\n",
    "\n",
    "    # Scale the input between [0,1]\n",
    "    input_mat = input_mat.astype(float)\n",
    "    input_mat -= np.min(input_mat)\n",
    "    input_mat /= np.max(input_mat)\n",
    "\n",
    "    # Calculate the mean of each channel for normalization\n",
    "    MEAN_ARRAY = np.ndarray(shape=(BAND,),dtype=np.float32)\n",
    "    for i in range(BAND):\n",
    "        MEAN_ARRAY[i] = np.mean(input_mat[:,:,i])\n",
    "\n",
    "    # Collect all available patches of each class from the given image\n",
    "    for i in range(OUTPUT_CLASSES):\n",
    "        CLASSES.append([])\n",
    "    print('Grenerating patches:')\n",
    "    if config.patch_mode == 'TopLeft':\n",
    "        report_progress(0, HEIGHT - PATCH_SIZE + 1)\n",
    "        for i in range(HEIGHT - PATCH_SIZE + 1):\n",
    "            report_progress(i+1, HEIGHT - PATCH_SIZE + 1)\n",
    "            for j in range(WIDTH - PATCH_SIZE + 1):\n",
    "                curr_inp = Patch_TopLeft(i,j) # shape of curr_inp is (C,H,W)\n",
    "                curr_tar = target_mat[i, j]\n",
    "                if(curr_tar!=0): #Ignore patches with unknown landcover type for the central pixel\n",
    "                    CLASSES[curr_tar-1].append(curr_inp)\n",
    "    if config.patch_mode == 'Center':\n",
    "        report_progress(0, HEIGHT)\n",
    "        for i in range(HEIGHT):\n",
    "            report_progress(i+1, HEIGHT)\n",
    "            for j in range(WIDTH):\n",
    "                curr_inp = Patch_Center(i,j) # shape of curr_inp is (C,H,W)\n",
    "                curr_tar = target_mat[i, j]\n",
    "                if(curr_tar!=0): #Ignore patches with unknown landcover type for the central pixel\n",
    "                    CLASSES[curr_tar-1].append(curr_inp)\n",
    "    report_progress_done()\n",
    "    sys.stdout.write('\\nInitial sample length per class:'.ljust(40))\n",
    "    for c  in CLASSES:\n",
    "        sys.stdout.write(str(len(c)).ljust(4) + ' ')\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "    # Only choose 9 classes which has more than 200 samples\n",
    "    IndianPines_list = [2,3,5,6,8,10,11,12,14]\n",
    "    for c in range(len(IndianPines_list)):\n",
    "        IndianPines_list[c] -= 1\n",
    "\n",
    "    # Make a train split with 200 data from 9 class, and test split with rest of data\n",
    "    for c in range(OUTPUT_CLASSES): \n",
    "        if c not in IndianPines_list:\n",
    "            TRAIN_PATCH.append([])\n",
    "            continue\n",
    "        class_population = len(CLASSES[c])\n",
    "        assert class_population > COUNT\n",
    "            \n",
    "        patches_of_current_class = CLASSES[c]\n",
    "        shuffle(patches_of_current_class)\n",
    "        \n",
    "        # Make training and test splits\n",
    "        TRAIN_PATCH.append(patches_of_current_class[:COUNT])\n",
    "        # Make test splits\n",
    "        TEST_PATCH.extend(patches_of_current_class[COUNT:])\n",
    "        test_split_size = len(TEST_PATCH[c])\n",
    "        TEST_LABELS.extend(np.full(test_split_size, c, dtype=int))\n",
    "    sys.stdout.write('Number of training samples per class:'.ljust(40))\n",
    "    for c in TRAIN_PATCH:\n",
    "        sys.stdout.write(str(len(c)).ljust(4) + ' ')\n",
    "    sys.stdout.write('\\n')\n",
    "    print('Length of testing data: {}\\n'.format(len(TEST_PATCH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the classes which do not have at least COUNT patches \n",
    "# in the training set and extract COUNT patches\n",
    "for i in range(OUTPUT_CLASSES):\n",
    "    if(len(TRAIN_PATCH[i])<COUNT):\n",
    "        tmp = TRAIN_PATCH[i]\n",
    "        for j in range(COUNT//len(TRAIN_PATCH[i])):\n",
    "            shuffle(TRAIN_PATCH[i])\n",
    "            TRAIN_PATCH[i] = TRAIN_PATCH[i] + tmp\n",
    "    shuffle(TRAIN_PATCH[i])\n",
    "    TRAIN_PATCH[i] = TRAIN_PATCH[i][:COUNT]\n",
    "sys.stdout.write('Number of training samples per class after oversampling:'.ljust(40))\n",
    "for c in TRAIN_PATCH:\n",
    "    sys.stdout.write(str(len(c)).ljust(4) + ' ')\n",
    "sys.stdout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
