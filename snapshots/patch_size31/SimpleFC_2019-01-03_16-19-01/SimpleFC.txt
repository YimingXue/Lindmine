#############################  PARAMETERS  ###################################
# cuda
	cuda: True
# train/test parameters	model_name: SimpleFC
	optimizer: SGD
	epochs: 120
	batch_size: 100
	seed: 75
	lr: 0.1
	weight_decay: 0.0001
# data preparation parameters
	patch_size: 31
	indianPines_band: 220
	indianPines_class: 16
# SimpleNet parameters
	conv1: 500
	conv2: 100
	fc1: 200
	fc2: 84
# SimpleFC parameters
	FC_1: 500
	FC_2: 350
	FC_3: 150
	FC_4: 16
##############################################################################

#############################  MODEL  ###################################

SimpleFC(
  (FC): Sequential(
    (0): Linear(in_features=211420, out_features=500, bias=True)
    (1): ReLU()
    (2): Linear(in_features=500, out_features=350, bias=True)
    (3): ReLU()
    (4): Linear(in_features=350, out_features=150, bias=True)
    (5): ReLU()
    (6): Linear(in_features=150, out_features=16, bias=True)
    (7): ReLU()
  )
  (Softmax): Softmax()
)
##############################################################################

Epoch 1/120| Time: 14.83s| Loss: 1.6527
Epoch 2/120| Time: 14.28s| Loss: 0.8889
Epoch 3/120| Time: 14.31s| Loss: 0.8467
Epoch 4/120| Time: 14.21s| Loss: 0.6560
Epoch 5/120| Time: 14.43s| Loss: 0.6864
Epoch 6/120| Time: 14.40s| Loss: 0.5153
Epoch 7/120| Time: 14.12s| Loss: 0.4441
Epoch 8/120| Time: 14.25s| Loss: 0.8205
Epoch 9/120| Time: 14.17s| Loss: nan
Epoch 10/120| Time: 14.14s| Loss: nan
>>--/home/xueyiming/PytorchWorkspace/snapshots/patch_size31/SimpleFC_2019-01-03_16-19-01/SimpleFC10.model model saved--<<
>>--/home/xueyiming/PytorchWorkspace/snapshots/patch_size31/SimpleFC_2019-01-03_16-19-01/SimpleFC10.model model loaded--<<
	Batch_idx: 0 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 1 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 2 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 3 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 4 | Loss: nan | AccuracyNumber: 11
	Batch_idx: 5 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 6 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 7 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 8 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 9 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 10 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 11 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 12 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 13 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 14 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 15 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 16 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 17 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 18 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 19 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 20 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 21 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 22 | Loss: nan | AccuracyNumber: 0
	Batch_idx: 23 | Loss: nan | AccuracyNumber: 0
Testing Loss: nan | Testing Accuracy: 0.4583 | Time: 14.83
